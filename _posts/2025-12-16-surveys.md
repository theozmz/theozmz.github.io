---
title: 'RAG相关surveys, benchmarks和methods'
date: 2025-12-16
permalink: /posts/2025/12/surveys/
tags:
  - RAG
  - memory
  - survey
---


这是RAG相关surveys, benchmarks和methods的汇总，分类方法仅供参考

# Introduction

基本流程：

1. **Divide the knowledge base:** Break the document corpus into smaller, manageable chunks.
2. **Create embeddings:** Apply an embedding model to transform these text chunks into vector embeddings, capturing their semantic meaning.
3. **Store in a vector database:** Save the embeddings in a vector database, enabling fast retrieval based on semantic similarity.
4. **Handle user queries:** Convert the user's query into an embedding using the same model that was applied to the text chunks.
5. **Retrieve relevant data:** Search the vector database for embeddings that closely match the query’s embedding based on semantic similarity.
6. **Enhance the prompt:** Incorporate the most relevant text chunks into the LLM’s prompt to provide valuable context for generating a response.
7. **Generate a response:** The LLM leverages the augmented prompt to deliver a response that is accurate and tailored to the user’s query.

1.**划分知识库：**将文档语料库分解为更小、可管理的块。
2.**创建嵌入：**应用嵌入模型将这些文本块转换为向量嵌入，捕捉它们的语义含义。
3.**存储在向量数据库中：**将嵌入保存在向量数据库，实现基于语义相似性的快速检索。
4.**处理用户查询：**使用应用于文本块的相同模型将用户的查询转换为嵌入。
5.**检索相关数据：**根据语义相似度在向量数据库中搜索与查询嵌入紧密匹配的嵌入。
6.**增强提示：**将最相关的文本块合并到LLM的提示中，为生成响应提供有价值的上下文。
7.**生成响应：**LLM利用增强提示提供准确且针对用户查询量身定制的响应。





# Survey and Benchmark 

## Benchmarks

**Benchmarking Large Language Models in Retrieval-Augmented Generation** \
*Jiawei Chen, Hongyu Lin, Xianpei Han, Le Sun* \
arXiv 2023. [[Paper](https://arxiv.org/abs/2309.01431)][[Github](https://github.com/chen700564/RGB)] \
4 Sep 2023 

**RECALL: A Benchmark for LLMs Robustness against External Counterfactual Knowledge**
*Yi Liu and Lianzhe Huang and Shicheng Li and Sishuo Chen and Hao Zhou and Fandong Meng and Jie Zhou and Xu Sun*
year={2023},
eprint={2311.08147},
archivePrefix={arXiv},
primaryClass={cs.CL},
url={https://arxiv.org/abs/2311.08147}, 
**ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems**
*Jon Saad-Falcon and Omar Khattab and Christopher Potts and Matei Zaharia*
year={2024},
eprint={2311.09476},
archivePrefix={arXiv},
primaryClass={cs.CL},
url={https://arxiv.org/abs/2311.09476}, 
**Ragas: Automated Evaluation of Retrieval Augmented Generation**
*Shahul Es and Jithin James and Luis Espinosa-Anke and Steven Schockaert*
year={2025},
eprint={2309.15217},
archivePrefix={arXiv},
primaryClass={cs.CL},
url={https://arxiv.org/abs/2309.15217}, 

**CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Models**
*Yuanjie Lyu and Zhiyu Li and Simin Niu and Feiyu Xiong and Bo Tang and Wenjin Wang and Hao Wu and Huanyong Liu and Tong Xu and Enhong Chen*
year={2024},
eprint={2401.17043},
archivePrefix={arXiv},
primaryClass={cs.CL},
url={https://arxiv.org/abs/2401.17043}, 

**FeB4RAG: Evaluating Federated Search in the Context of Retrieval Augmented Generation**
*Shuai Wang and Ekaterina Khramtsova and Shengyao Zhuang and Guido Zuccon*
year={2024},
eprint={2402.11891},
archivePrefix={arXiv},
primaryClass={cs.IR},
url={https://arxiv.org/abs/2402.11891}, 

**CodeRAG-Bench: Can Retrieval Augment Code Generation?**
*Zora Zhiruo Wang and Akari Asai and Xinyan Velocity Yu and Frank F. Xu and Yiqing Xie and Graham Neubig and Daniel Fried*
year={2025},
eprint={2406.14497},
archivePrefix={arXiv},
primaryClass={cs.SE},
url={https://arxiv.org/abs/2406.14497}, 
**Long$^2$RAG: Evaluating Long-Context & Long-Form Retrieval-Augmented Generation with Key Point Recall**
*Zehan Qi and Rongwu Xu and Zhijiang Guo and Cunxiang Wang and Hao Zhang and Wei Xu*
year={2025},
eprint={2410.23000},
archivePrefix={arXiv},
primaryClass={cs.CL},
url={https://arxiv.org/abs/2410.23000}, 

## Surveys

**A Survey on Retrieval-Augmented Text Generation**
*Huayang Li and Yixuan Su and Deng Cai and Yan Wang and Lemao Liu*
year={2022},
eprint={2202.01110},
archivePrefix={arXiv},
primaryClass={cs.CL},
url={https://arxiv.org/abs/2202.01110}, 

**Retrieving Multimodal Information for Augmented Generation: A Survey**
*Ruochen Zhao and Hailin Chen and Weishi Wang and Fangkai Jiao and Xuan Long Do and Chengwei Qin and Bosheng Ding and Xiaobao Guo and Minzhi Li and Xingxuan Li and Shafiq Joty*
year={2023},
eprint={2303.10868},
archivePrefix={arXiv},
primaryClass={cs.CL},
url={https://arxiv.org/abs/2303.10868}, 

**Retrieval-Augmented Generation for Large Language Models: A Survey**
*Yunfan Gao and Yun Xiong and Xinyu Gao and Kangxiang Jia and Jinliu Pan and Yuxi Bi and Yi Dai and Jiawei Sun and Meng Wang and Haofen Wang*
year={2024},
eprint={2312.10997},
archivePrefix={arXiv},
primaryClass={cs.CL},
url={https://arxiv.org/abs/2312.10997}, 

**A Survey of Context Engineering for Large Language Models**
*Lingrui Mei and Jiayu Yao and Yuyao Ge and Yiwei Wang and Baolong Bi and Yujun Cai and Jiazhi Liu and Mingyu Li and Zhong-Zhi Li and Duzhen Zhang and Chenlin Zhou and Jiayi Mao and Tianze Xia and Jiafeng Guo and Shenghua Liu*
year={2025},
eprint={2507.13334},
archivePrefix={arXiv},
primaryClass={cs.CL},
url={https://arxiv.org/abs/2507.13334}, 

**Context Engineering 2.0: The Context of Context Engineering**
*Qishuo Hua and Lyumanshan Ye and Dayuan Fu and Yang Xiao and Xiaojie Cai and Yunze Wu and Jifan Lin and Junfei Wang and Pengfei Liu*
year={2025},
eprint={2510.26493},
archivePrefix={arXiv},
primaryClass={cs.AI},
url={https://arxiv.org/abs/2510.26493}, 



# Methods

## Retrieval-enhanced LLMs

**Active Retrieval Augmented Generation** \
*Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig* \
EMNLP 2023 - May 2023 [[Paper](https://arxiv.org/abs/2305.06983)] [[Github](https://github.com/jzbjyb/flare)] \

**摘要概述**
论文提出“Active Retrieval Augmented Generation”，研究在长文本生成过程中主动、多轮检索以减少幻觉并提升事实性。作者提出 Forward-Looking Active REtrieval augmented generation (FLARE)：先预测下一句，若其中存在低置信度 token，则用预测句生成检索查询，并基于检索结果重新生成该句。 他们在 4 个长篇知识密集型生成任务/数据集上系统评测，FLARE 在这些任务上取得优于或可比的表现。​

**关键方法**

- 提出通用的“主动检索式生成”框架，显式建模“何时检索、检什么”而不是单次检索。
- FLARE 用“前瞻句预测 + 置信度检测”来触发检索，并以预测句作为查询，用检索文档重写对应句子。

**主要数据集/任务**

- 4 个长篇、知识密集型生成任务，包括长篇问答和基于文档的生成任务（原文未在摘要逐一列名，但均为 open-domain/知识密集场景，如长文 QA、摘要/解释生成等）。





**Adaptive Retrieval without Self-Knowledge? Bringing Uncertainty Back Home**  \
Viktor Moskvoretskii, Maria Lysyuk, Mikhail Salnikov, Nikolay Ivanov, Sergey Pletenev, Daria Galimzianova, Nikita Krayko, Vasily Konovalov, Irina Nikishina, Alexander Panchenko \
arxiv – Jan 2025 [[paper](https://arxiv.org/abs/2501.12835)]

**摘要概述**
论文关注“自适应检索”中如何利用模型自身的不确定性，而非显式构造自知识数据集。作者提出一种“将不确定性带回模型内部”的框架，用模型输出的不确定性信号来决定是否和如何调用检索模块，以减少不必要检索并缓解噪声文档干扰。该方法在多种 QA 等知识密集任务上验证，显示可以在减少检索开销的同时维持或提升性能。​

**关键方法**

- 不显式构造“已见/未见问题”自知识标注，而是直接从模型的 token 或答案级不确定性估计中推断是否需要检索。
- 设计统一的自适应策略：当不确定性高时激活检索并动态调整检索强度，否则依赖 parametric knowledge。

**主要数据集/任务**

- 多个开放域 QA 与知识密集任务数据集，用于比较“固定检索 vs 自适应检索 vs 无检索”的表现；具体名称（例如 NQ、TriviaQA 等）在主文中列出但不在公开摘要中一一给出。





**DFA-RAG: Conversational Semantic Router for Large Language Model with Definite Finite Automaton**  \
Yiyou Sun, Junjie Hu, Wei Cheng, Haifeng Chen \
ICML 24 – Feb 2024 [[paper](https://arxiv.org/abs/2402.04411)]

**摘要概述**
论文提出 DFA-RAG，一种基于确定有限自动机（DFA）的语义路由框架，用于多轮对话场景下动态选择检索与工具。系统将对话语义映射到 DFA 状态，并根据状态转移选择相应检索器或子模块，从而控制 LLM 的对话流程。作者展示该方法在对话式 QA 与工具调用场景中可以提高准确性与可控性。​

**关键方法**

- 用 DFA 建模多轮对话中的“语义状态机”，每个状态绑定特定检索/生成策略。
- 将路由逻辑显式化：LLM 负责理解语义并决定状态转移，而检索与回答由对应模块执行。

**主要数据集/任务**

- 多轮对话 QA 与工具增强对话数据集，用于评估路由准确率和下游任务性能（具体数据集名称在正文中，如 MultiWOZ 类对话或自构对话-RAG基准）。





**Chain-of-Note: Enhancing Robustness in Retrieval-Augmented Language Models** \
*Wenhao Yu, Hongming Zhang, Xiaoman Pan, Kaixin Ma, Hongwei Wang, Dong Yu* \
arxiv - Nov 2023 [[Paper](https://arxiv.org/abs/2311.09210)] 

**摘要概述**
论文指出：RAG 在面对噪声或不相关文档时会被误导，且难以判断“我不知道”应回答 unknown。 为此提出 Chain-of-Note (CoN) 框架：对每篇检索文档生成顺序“阅读笔记”，用笔记评估文档与问题的相关性，并整合进最终答案。实验证明，装备 CoN 的 RALM 在多个开放域 QA 基准上显著优于标准 RAG，尤其在全噪声检索或训练外实时问题上有更高 EM 和拒答率。​

**关键方法**

- 为每篇文档生成“Chain-of-Noting”笔记，结构化记录要点并标记与问题的相关性。
- 在聚合阶段，模型先基于笔记过滤无关/误导文档，再决定使用内在知识或回答 unknown。

**主要数据集/任务**

- 四个开放域 QA 基准（如 NQ、TriviaQA、WebQuestions、HotpotQA 一类开放域问答），在噪声检索和“训练后实时问题”场景下评估。CoN 在全噪声文档条件下 EM 平均提升约 +7.9，在实时外部问题上的拒答率提升约 +10.5。





**REST: Retrieval-Based Speculative Decoding** \
*Zhenyu He, Zexuan Zhong, Tianle Cai, Jason D Lee, Di He* \
arXiv - Nov 2023 [[Paper](https://arxiv.org/abs/2311.08252)][[Github](https://github.com/fasterdecoding/rest)]

**摘要概述**
论文提出 REST：一种“检索式 speculative decoding”算法，用非参数数据存储替换传统的小草稿 LM 来加速大模型推理。 REST 利用上下文在 datastore 中检索“上下文–延续”对，拼接出草稿 token 序列，再由大模型一次性验证并接受尽可能多的 token，从而在保持输出质量的同时显著提速。实验在代码生成（HumanEval）和多轮对话评测（MT-Bench）上表明 REST 能在 12GB 规模的 datastore 条件下取得可观加速与精度保持。​

**关键方法**

- 构建基于历史文本的“datastore”，存储大量 context–continuation 对，推理时以当前上下文检索匹配的延续序列作为草稿。
- 草稿再由目标大模型集中验证，批量接受前缀 token，从而减少每个输出 token 所需的模型前向次数。

**主要数据集/任务**

- HumanEval：代码生成基准，用 Pass@k 等指标衡量质量与推理加速比。
- MT-Bench：多轮对话质量基准，用于评估在对话生成中的加速与质量损失。





**Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection**  
*Anonymous*  
ICLR 24 – Oct 2023 [[paper](https://openreview.net/forum?id=hSyW5go0v8)]

**摘要概述**
Self-RAG 提出让模型“自我检索、自我生成并自我批判”的统一框架，用自反思信号训练检索与生成模块。 模型在生成过程中先预测是否需要检索、检索什么文档，然后生成答案并对自己生成的内容进行 critique，批判结果再回流指导检索选择与最终输出。该方法在开放域 QA、长文生成等任务上提高事实性并减少幻觉。​

**关键方法**

- 用统一的“自反思评分”去联合训练检索器和生成器，使其学会何时检索以及怎样利用证据。
- 增加 critique 阶段：模型对自身答案及其证据一致性进行评价，并可触发再次检索或修改答案。

**主要数据集/任务**

- 多个知识密集任务基准（开放域 QA、维基百科型长文解释等），在 hallucination 率与事实性指标上对比标准 RAG。





**Self-Knowledge Guided Retrieval Augmentation for Large Language Models** \
*Yile Wang, Peng Li, Maosong Sun, Yang Liu* \
arXiv - Oct 2023 [[Ppaer](https://arxiv.org/abs/2310.05002)]

**摘要概述**
论文发现检索到的外部知识并非总是有益，甚至可能损伤原始回答质量。 为更好地结合内部与外部知识，他们提出 Self-Knowledge guided Retrieval augmentation (SKR)：先显式“引出”模型对每个问题的自知识（是否掌握），然后只在“自认不知道”的问题上触发检索。 实验在 5 个数据集上，用 InstructGPT 和 ChatGPT 表明 SKR 相比链式思维和全量 RAG 都更优。​

**关键方法**

- 设计多种自知识 elicitation 策略：直接提示、in-context learning、训练小分类器或用 kNN 预测“已见/未见”，对每个问题打上“需要检索/不需要检索”标签。
- 在推理时依据自知识标签自适应调用检索模块，从而减少无效或有害检索。

**主要数据集/任务**

- 在 5 个 QA 数据集上评估（包含开放域与知识密集问答），比较“无检索、全检索、SKR 自适应检索”的差异，SKR 在准确率和利用检索效率上全面占优。





**Retrieval meets Long Context Large Language Models** \
*Peng Xu, Wei Ping, Xianchao Wu, Lawrence McAfee, Chen Zhu, Zihan Liu, Sandeep Subramanian, Evelina Bakhturina, Mohammad Shoeybi, Bryan Catanzaro* \
arxiv - Oct 2023 [[Paper](https://arxiv.org/abs/2310.03025)]

**摘要概述**
论文系统比较“扩长上下文窗口”和“检索增强”两种方案，并探讨能否结合两者优势。 他们用一款 43B 的专有 GPT 和 Llama2-70B，在多种长上下文任务中评估：结果显示，即便只用 4k 上下文的 LLM，配合简单检索也能达到 16k 上下文 LLM 的接近性能，同时更快。​

**关键方法**

- 构建统一实验框架，对“固定长上下文”、“检索 + 短上下文”、“检索 + 长上下文”进行对照。
- 分析检索数量、窗口长度与任务表现和推理成本的关系，为实践中如何选型提供经验结论。

**主要数据集/任务**

- 多种长上下文任务：单/多文档问答、基于脚本的问答（如 QASP、NQA、MSQ、HQA、MFQA 等），以及长文摘要类任务。
- 使用 Llama2-70B-4k vs 32k 比较，发现“4k + 检索”可与“32k 长上下文”性能相当。





**DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines**  
*Omar Khattab, Arnav Singhvi, Paridhi Maheshwari, Zhiyuan Zhang, Keshav Santhanam, Sri Vardhamanan, Saiful Haq, Ashutosh Sharma, Thomas T. Joshi, Hanna Moazam, Heather Miller, Matei Zaharia, Christopher Potts*  
arXiv – Oct 2023 [[paper](https://arxiv.org/abs/2310.03714)] [[code](https://github.com/stanfordnlp/dspy)]

**摘要概述**
DSPy 提出一种编程模型，把 LM 管线抽象为“文本转换图”，在图中通过“声明式模块”调用 LM。 模块是可参数化的，可通过收集示例学习如何组合 prompt、微调、检索和推理技巧；作者设计了一个编译器，给定目标度量自动优化任意 DSPy 管线。 在两个案例中，简洁的 DSPy 程序就能自动“自举”出多步检索与推理链，在数学文字题、多跳检索和复杂问答等任务上，相比标准 few-shot 提示提升 25–65%，并接近或超过专家设计的 prompt 链。​

**关键方法**

- 将 LM 调用封装为“模块”，通过 signatures 声明输入输出类型，形成可编译的文本转换图。
- 编译器通过搜索/学习示例自动优化模块参数和 prompts，从而让整个 RAG/推理流程“自我改进”。

**主要数据集/任务**

- 数学文字题、多跳检索问答、复杂 QA 和 agent-loop 控制等任务；实验中一般从官方训练集抽取 200–300 个样本作为 train/dev，并在测试集上评估。





**Adaptive Chameleon  or Stubborn Sloth: Revealing the Behavior of Large Language Models in Knowledge Conflicts**  
*Jian Xie, Kai Zhang, Jiangjie Chen, Renze Lou, Yu Su*  
ICLR 24 – May 2023 [[paper](https://arxiv.org/abs/2305.13300)] [[code](https://github.com/OSU-NLP-Group/LLM-Knowledge-Conflict)]

**摘要概述**
论文系统研究 LLM 在“知识冲突”场景（内部记忆 vs 外部证据冲突）下的行为。 作者提出框架来提取高质量 parametric memory，并构造对应“counter-memory”，据此开展一系列受控实验。 结论显示：一方面，如果外部证据是唯一且连贯的，LLM 很容易接受并覆盖自身记忆；另一方面，当外部证据包含少量与内部记忆一致的信息时，模型表现出强烈确认偏误，更倾向选择与自身记忆一致的部分。​

**关键方法**

- 提出“parametric memory 提取 + counter-memory 构造”的系统流程，构建可控的知识冲突样例。
- 在不同冲突设置下测试多种 LLM，并分析其对外部证据的接纳度与确认偏误。

**主要数据集/任务**

- 基于上述框架构造的 ConflictQA 风格数据集（由内部知识与冲突证据组合而成），公开在 GitHub 上供后续研究。





**REPLUG: Retrieval-Augmented Black-Box Language Models**  
*Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, Wen-tau Yih*  
arXiv – Jan 2023 [[paper](https://arxiv.org/abs/2301.12652)]

**摘要概述**
REPLUG 提出一种“黑盒检索增强语言建模框架”，把 LM 当成完全黑盒，只通过“前置检索文档到输入”进行增强。 与以往需要修改模型结构引入 cross-attention 的方法不同，REPLUG 仅在输入前拼接检索文档，因此可无缝用于任意现成 LM。 作者进一步用 LM 自身的语言建模分数监督 retriever，使检索更倾向帮助 LM 提高预测质量；在语言建模、开放域 QA 和 MMLU 上，REPLUG 显著提升 GPT、OPT、BLOOM 等模型性能。​

**关键方法**

- 将检索文档简单拼接到 prompt 前，不改动 LM 权重或结构；再用 LM 的 log-likelihood 作为信号来训练/微调 retriever（Language-Model-Supervised Retrieval）。
- 设计多文档 ensemble 方案，应对 LM 上下文长度限制，通过多次调用并集成输出概率。

**主要数据集/任务**

- 语言建模（如 Pile 子集）、开放域 QA 数据集，以及 MMLU 知识测评数据集，用于验证在黑盒设定下的增益。





**Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks** 
*Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, Douwe Kiela* 
NeurIPS 2020 - May 2020 [[Paper](https://arxiv.org/abs/2005.11401)]

**摘要概述**
该论文首次系统提出“Retrieval-Augmented Generation (RAG)”框架：将预训练 seq2seq 模型作为 parametric memory，与基于 dense 向量索引的 Wikipedia 非参数 memory 结合。 作者比较两种 RAG 形式：一种在整个生成序列上条件于同一批检索段落，另一种允许运行中为每个 token 选择不同段落。 他们在多种知识密集 NLP 任务上微调 RAG，在三个开放域 QA 任务上达到 SOTA；同时 RAG 在生成任务中比纯 parametric 的 seq2seq baseline 更具体、多样且更具事实性。​

**关键方法**

- 使用预训练 neural retriever 建立 Wikipedia dense 索引，并与 BART 等 seq2seq 模型联合训练，形成 end-to-end RAG。
- 设计 RAG-Sequence 与 RAG-Token 两种架构，分别对应“固定检索集合”和“逐 token 动态检索”。

**主要数据集/任务**

- 多个开放域 QA 和知识密集任务；三项开放域 QA 上取得 SOTA（典型包括 Natural Questions、WebQuestions、CuratedTREC 等）。





**Reasoning RAG【双系统智检】**

> **双系统智检**：像一位既会“快思”又会“慢想”的侦探，把RAG拆成两条赛道——System 1用模块化流水线闪电出招，System 2让LLM自己决定何时、如何检索与反思；两者配合，既能稳定落地，又能应对开放世界的复杂推理。
>

* 时间：2025.06.12
* 论文：[Reasoning RAG via System 1 or System 2: A Survey on Reasoning Agentic Retrieval-Augmented Generation](https://arxiv.org/abs/2506.10408)

Reasoning RAG 以“快思”式固定模块流水线与“慢想”式自主检索双轨并行。论文系统梳理两大范式下 Self-RAG、ReAct、DeepResearcher 等代表工作的架构、策略、工具及奖励设计，并指出现实 Web 环境端到端 RL 训练是通往可信、高效、开放世界推理 RAG 的关键。





**RAT【思考者】**

> **思考者**：像个善于反思的导师，不是一次性得出结论，而是先有初步想法，然后利用检索到的相关信息，不断审视和完善每一步推理过程，让思维链条更加严密可靠。
>

* 时间：2024.03.08
* 论文：[RAT: Retrieval Augmented Thoughts Elicit Context-Aware Reasoning in Long-Horizon Generation](https://arxiv.org/abs/2403.05313)
* 项目：[https://github.com/CraftJarvis/RAT](https://github.com/CraftJarvis/RAT)
* 参考：[https://mp.weixin.qq.com/s/TqmY4ouDuloE2v-iSJB_-Q](https://mp.weixin.qq.com/s/TqmY4ouDuloE2v-iSJB_-Q)

RAT（检索增强思维）在生成初始零样本思维链（CoT）后，利用与任务查询、当前和过去思维步骤相关的检索信息逐个修订每个思维步骤，RAT可显著提高各种长时生成任务上的性能。





**RAFT【开卷高手】**

> **开卷高手**：像个优秀的考生，不仅会找对参考资料，还能准确引用关键内容，并清晰地解释推理过程，让答案既有据可循又合情合理。
>

* 时间：2024.03.15
* 论文：[RAFT: Adapting Language Model to Domain Specific RAG](https://arxiv.org/abs/2403.10131)
* 参考：[https://mp.weixin.qq.com/s/PPaviBpF8hdviqml3kPGIQ](https://mp.weixin.qq.com/s/PPaviBpF8hdviqml3kPGIQ)

RAFT旨在提高模型在特定领域内的“开卷”环境中回答问题的能力，通过训练模型忽略无关文档，并逐字引用相关文档中的正确序列来回答问题，结合思维链式响应，显著提升了模型的推理能力。





**Adaptive-RAG【因材施教】**

> **因材施教**：面对不同难度的问题，它会智能地选择最合适的解答方式。简单问题直接回答，复杂问题则会查阅更多资料或分步骤推理，就像一个经验丰富的老师，懂得根据学生的具体问题调整教学方法。
>

* 时间：2024.03.21
* 论文：[Adaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language Models through Question Complexity](https://arxiv.org/abs/2403.14403)
* 项目：[https://github.com/starsuzi/Adaptive-RAG](https://github.com/starsuzi/Adaptive-RAG)
* 参考：[https://mp.weixin.qq.com/s/sxAu8xahY-GthS--nfgmjg](https://mp.weixin.qq.com/s/sxAu8xahY-GthS--nfgmjg)

Adaptive-RAG根据查询的复杂程度动态选择最适合的检索增强策略，从最简单到最复杂的策略中动态地为LLM选择最合适的策略。这个选择过程通过一个小语言模型分类器来实现，预测查询的复杂性并自动收集标签以优化选择过程。这种方法提供了一种平衡的策略，能够在迭代式和单步检索增强型 LLMs 以及无检索方法之间无缝适应，以应对一系列查询复杂度。





**RAE【智能编辑】**

> **智能编辑**：像个细心的新闻编辑，不仅会深入挖掘相关事实，还能通过连环推理找出容易被忽略的关键信息，同时懂得删减冗余内容，确保最终呈现的信息既准确又精炼，避免"说得天花乱坠却不靠谱"的问题。
>

* 时间：2024.03.28
* 论文：[Retrieval-enhanced Knowledge Editing in Language Models for Multi-Hop Question Answering](https://arxiv.org/abs/2403.19631)
* 项目：[https://github.com/sycny/RAE](https://github.com/sycny/RAE)
* 参考：[https://mp.weixin.qq.com/s/R0N8yexAlXetFyCS-W2dvg](https://mp.weixin.qq.com/s/R0N8yexAlXetFyCS-W2dvg)

RAE（多跳问答检索增强模型编辑框架）首先检索经过编辑的事实，然后通过上下文学习来优化语言模型。基于互信息最大化的检索方法利用大型语言模型的推理能力来识别传统基于相似性的搜索可能会错过的链式事实。此外框架包括一种修剪策略，以从检索到的事实中消除冗余信息，这提高了编辑准确性并减轻了幻觉问题。





**RAGCache【仓储员】**

> **仓储员**：像大型物流中心一样，把常用知识放在最容易取的货架上。懂得把经常用的包裹放在门口，把不常用的放在后仓，让取货效率最大化。
>

* 时间：2024.04.18
* 论文：[RAGCache: Efficient Knowledge Caching for Retrieval-Augmented Generation](https://arxiv.org/abs/2404.12457)
* 参考：[https://mp.weixin.qq.com/s/EOf51zoycmUCKkIo8rPsZw](https://mp.weixin.qq.com/s/EOf51zoycmUCKkIo8rPsZw)

RAGCache是一种为RAG量身定制的新型多级动态缓存系统，它将检索到的知识的中间状态组织在知识树中，并在GPU和主机内存层次结构中进行缓存。RAGCache提出了一种考虑到LLM推理特征和RAG检索模式的替换策略。它还动态地重叠检索和推理步骤，以最小化端到端延迟。





**MRAG【八爪鱼】**

> **八爪鱼**：不是只长一个脑袋死磕问题，而是像章鱼一样长出多个触角，每个触角负责抓取一个角度。简单说，这就是AI版的"一心多用"。
>

* 时间：2024.06.07
* 论文：[Multi-Head RAG: Solving Multi-Aspect Problems with LLMs](https://arxiv.org/abs/2406.05085)
* 项目：[https://github.com/spcl/MRAG](https://github.com/spcl/MRAG)
* 参考：[https://mp.weixin.qq.com/s/WFYnF5UDlmwYsWz_BMtIYA](https://mp.weixin.qq.com/s/WFYnF5UDlmwYsWz_BMtIYA)

现有的 RAG 解决方案并未专注于可能需要获取内容差异显著的多个文档的查询。此类查询经常出现，但具有挑战性，因为这些文档的嵌入在嵌入空间中可能相距较远，使得难以全部检索到它们。本文介绍了多头 RAG（MRAG），这是一种新颖的方案，旨在通过一个简单而强大的想法来填补这一空白：利用 Transformer 多头注意力层的激活，而非解码器层，作为获取多方面文档的键。其驱动动机是不同的注意力头可以学习捕获不同的数据方面。利用相应的激活会产生代表数据项和查询各个层面的嵌入，从而提高复杂查询的检索准确性。





**PlanRAG【战略家】**

> **战略家**：先制定完整作战计划，再根据规则和数据分析局势，最后做出最佳战术决策。
>

* 时间：2024.06.18
* 论文：[PlanRAG: A Plan-then-Retrieval Augmented Generation for Generative Large Language Models as Decision Makers](https://arxiv.org/abs/2406.12430)
* 项目：[https://github.com/myeon9h/PlanRAG](https://github.com/myeon9h/PlanRAG)
* 参考：[https://mp.weixin.qq.com/s/q3x2jOFFibyMXHA57sGx3w](https://mp.weixin.qq.com/s/q3x2jOFFibyMXHA57sGx3w)

PlanRAG研究如何利用大型语言模型解决复杂数据分析决策问题的方案，通过定义决策问答（Decision QA）任务，即根据决策问题Q、业务规则R和数据库D，确定最佳决策d。PlanRAG首先生成决策计划，然后检索器生成数据分析的查询。





**FoRAG【作家】**

> **作家**：先列写作大纲构思文章框架，再逐段扩充完善内容。同时还配备了一个"编辑"，通过仔细的事实核查和修改建议，帮助完善每个细节，确保作品的质量。
>

* 时间：2024.06.19
* 论文：[FoRAG: Factuality-optimized Retrieval Augmented Generation for Web-enhanced Long-form Question Answering](https://arxiv.org/abs/2406.13779)
* 参考：[https://mp.weixin.qq.com/s/7uqZ5U10Ec2Pa7akCLCJEA](https://mp.weixin.qq.com/s/7uqZ5U10Ec2Pa7akCLCJEA)

FoRAG提出了一种新颖的大纲增强生成器，在第一阶段生成器使用大纲模板，根据用户查询和上下文草拟答案大纲，第二阶段基于生成的大纲扩展每个观点，构建最终答案。同时提出一种基于精心设计的双精细粒度RLHF框架的事实性优化方法，通过在事实性评估和奖励建模两个核心步骤中引入细粒度设计，提供了更密集的奖励信号。





## RAG Instruction Tuning 

**RA-DIT: Retrieval-Augmented Dual Instruction Tuning**  
*Anonymous*  
ICLR 24 – Oct 23 [[paper](https://openreview.net/forum?id=22OTbutug9)]

**核心内容与目标**
RA-DIT 提出一种“轻量双阶段微调”方法，把任意现成 LLM 改造成效果强的 RAG 模型，而不需要在预训练阶段改结构或重新大规模预训练。​

**关键方法**

- Dual Instruction Tuning：
  1）在带检索上下文的指令数据上微调 LLM，使其更好使用检索到的信息；
  2）再用 LLM 的偏好信号微调 retriever，使其返回更符合 LLM 需求的结果。​
- 通过混合“语料检索数据 + 多任务 instruction 数据”，控制检索器在真实语料与指令任务之间的适配，得到更鲁棒的 RALM。

**主要数据集/任务**

- 知识密集型 zero-/few-shot 基准：MMLU、Natural Questions (NQ)、TriviaQA、ELI5 等，用于评价知识问答与推理能力。
- RA-DIT 65B 在上述基准上 0-shot 平均可比未微调的 RALM 提升约 8–9 个点，5-shot 提升约 1–2 个点。





**InstructRetro: Instruction Tuning post Retrieval-Augmented Pretraining**  
*Boxin Wang, Wei Ping, Lawrence McAfee, Peng Xu, Bo Li, Mohammad Shoeybi, Bryan Catanzaro* \
arXiv -  Oct 23 [[paper](https://openreview.net/forum?id=4stB7DFLp6)]

**核心内容与目标**
InstructRetro 研究“先做检索增强预训练，再做 instruction tuning”的范式，目标是让大模型既具备 RAG 风格的记忆/检索能力，又能在指令任务上实现强的零样本泛化。​

**关键方法**

- Retro 风格的检索增强预训练：使用 BERT 等编码器对超大规模语料建立向量索引，训练自回归 LM 在生成时条件于 k 个最近邻片段，从而在预训练阶段就习得“用检索补知识”的能力。
- 在此基础上进行标准 instruction tuning，但注意到指令数据并不总有高质量邻居，因而对检索管线做了清洗和任务特定语料构建，使检索在指令阶段仍然有用而非噪声。

**主要数据集/任务**

- 8 个短答 QA/阅读理解任务、4 个困难长问答任务、3 个摘要任务，用于比较“指令 GPT”与“指令 Retro”的差异。
- InstructRetro 43B 相比同数据训练的 GPT，在短答 QA 上平均提升约 7%，在 4 个长问答任务上提升约 10%，在 3 个摘要任务上提升约 16%。






## RAG In-Context Learning 

**In-Context Retrieval-Augmented Language Models**  
*Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, Yoav Shoham*  
AI21 Labs – Jan 2023 [[paper](https://uploads-ssl.webflow.com/60fd4503684b466578c0d307/63c6c20dec4479564db21819_NEW_In_Context_Retrieval_Augmented_Language_Models.pdf)] [[code](https://github.com/AI21Labs/in-context-ralm)]

**核心内容与目标**
这篇工作提出“纯 in-context 的 RALM”：完全不改动 LM 结构、不再训练 LM，仅通过在输入前拼接检索到的 grounding 文档来增强语言建模与 QA 能力。​

**关键方法**

- 直接将多个检索文档拼接到用户输入之前，LM 只作为黑盒使用；通过改进检索与排序器（如使用 BERT、Contriever、Spider 等）专门为 in-context 设定优化检索质量。
- 分析“检索位置、文档数量、排序方式”对困惑度与 QA 准确率的影响，证明在不同 LM 规模和语料上，这种极简 RALM 仍能带来稳定收益。

**主要数据集/任务**

- 5 个语言建模数据集：WikiText-103、RealNews，以及 The Pile 中的 ArXiv、StackExchange、FreeLaw 子集，用于评估困惑度降低情况。
- 2 个开放域 QA 数据集，使用训练集构成检索库，将文档切分为约 100 词的小段（输入截断到 256 token），验证 QA 性能提升。






## RAG Embeddings 

**RegaVAE: A Retrieval-Augmented Gaussian Mixture Variational Auto-Encoder for Language Modeling** \
*Jingcheng Deng, Liang Pang, Huawei Shen, Xueqi Cheng* \
EMNLP 2023 - Oct 2023 [[Paper](https://arxiv.org/abs/2310.10567)][[Github](https://github.com/TrustedLLM/RegaVAE)] 

**核心想法**
提出 RegaVAE，将整个语料编码到 VAE 的连续隐空间，并把传统的单高斯先验扩展为高斯混合先验，使检索与生成在隐空间中完成，而不是直接拼接原文，从而更好利用“当前+未来”信息、减少上下文噪声与长度限制，在多个文本生成任务上提升质量并减轻幻觉。​

**关键方法**

- 用 VAE 对文本进行编码，隐变量同时捕获 source 与 target（未来 token）的信息，支持“面向未来”的检索。
- 将 LM 的检索-生成范式写成概率形式，把先验从单高斯扩展为高斯混合，通过从混合成分中检索相关隐变量来聚合信息，而不是直接拼接长文档。
- 给出可优化的上界目标，用于联合训练检索和生成模块，并在与纯 VAE 对比中展示多模态高斯混合带来的多样性与质量提升。

**主要数据集**
论文在多个文本生成数据集上实验，公开代码仓库中提到可下载 3 个语料（常见是新闻或对话类生成任务），但原文未在摘要中逐一列名，需要查正文表格获取具体名称（如新闻生成、对话生成等标准基准）。





**Text Embeddings Reveal (Almost) As Much As Text** \
*John X. Morris, Volodymyr Kuleshov, Vitaly Shmatikov, Alexander M. Rush*  \
EMNLP 2023 - Oct 2023 [[Paper](https://arxiv.org/abs/2310.06816?ref=upstract.com)][[Github](https://github.com/jxmorris12/vec2text)] 

**核心想法**
研究“embedding 反演”：给定语句的 dense embedding，能否重建原文本。提出一个多步 controlled generation 方法，对生成文本不断 re-embed 并迭代纠正，使得最终嵌入接近目标，结果显示对 32-token 输入可精确恢复 92% 的文本，说明文本嵌入在隐私上几乎和原文一样敏感。​

**关键方法**

- 将 embedding 反演表述为控制生成任务：生成文本，使其嵌入接近目标向量，使用多轮“生成 → 嵌入 → 纠错”的迭代过程，而不是一次性解码。
- 训练 vec2text 模型解码两种 SOTA embedding 模型的向量，包含专门的纠错步骤来逐步逼近目标嵌入。
- 在临床笔记上展示模型可以恢复关键个人信息（如姓名），强调嵌入的隐私风险。

**主要数据集**

- 维基百科片段（长度 32 token）用于系统性评估 BLEU 与精确匹配率。
- 临床笔记数据集，用于评估从嵌入恢复患者敏感信息的可能性（具体来自既有医疗 NLP 语料）。





**Jina Embeddings 2: 8192-Token General-Purpose Text Embeddings for Long Documents** \
*Michael Günther, Jackmin Ong, Isabelle Mohr, Alaeddine Abdessalem, Tanguy Abel, Mohammad Kalim Akram, Susana Guzman, Georgios Mastrapas, Saba Sturua, Bo Wang, Maximilian Werk, Nan Wang, Han Xiao* \
arXiv - Oct 2023. [[Paper](https://arxiv.org/abs/2310.19923)][[Model](https://huggingface.co/jinaai/jina-embeddings-v2-small-en)] 

**核心想法**
提出 Jina Embeddings 2，一个支持 8192 token 输入的开源文本嵌入模型，突破常见 512-token 限制，适合长文档检索、聚类和重排序，在 MTEB 上达到 SOTA 或接近 OpenAI ada-002 的表现，并表明长上下文可以提升 NarrativeQA 等任务效果。​

**关键方法**

- 在 BERT 框架中双向引入 ALiBi，使 encoder 可以处理长达 8192 token 的输入，并在其上进行预训练。
- 在数亿文本对上进行无监督 + 有监督微调，并使用 Sentence-BERT 风格的 mean pooling 将 token 表示汇聚为单一向量。
- 系统性评估长文本场景下的检索与聚类性能，展示长上下文对嵌入任务的优势。

**主要数据集**

- MTEB（Massive Text Embedding Benchmark）中的多种检索、聚类和分类子任务数据集。
- NarrativeQA 等长文本问答 / 理解数据，用于分析长上下文带来的增益。
- 约 40 个来源构成的大规模文本对语料（包括 title–abstract 配对等）用于训练嵌入模型。





## RAG Simulators

**KAUCUS: Knowledge Augmented User Simulators for Training Language Model Assistants** \
*Kaustubh D. Dhole* \
Simulation of Conversational Intelligence in Chat, EACL 2024 [[Paper](https://arxiv.org/abs/2401.16454)]

**核心想法**
提出 KAUCUS 框架，构建可利用外部知识的用户模拟器，通过检索增强（RAG）或摘要控制的 GPT-J 模拟器生成多样的用户–助手对话数据，用于训练更有帮助的多轮指令跟随助手。​

**关键方法**

- 提出一个通用流程：收集人机对话示例 → 用外部知识扩展数据 → 基于 GPT-J 训练两类模拟器（检索增强模拟器和摘要控制模拟器），生成合成交互数据。
- 在模拟器中集成检索或摘要模块，使合成用户请求能反映互联网上多样、最新的知识分布。
- 使用奖励模型与偏好模型评估生成数据，证明用这些数据训练的助手在有用性上优于基线助手。

**主要数据集**

- 人工收集的基础用户–助手对话（例如 Anthropic 等开放指令数据，文中以 Anthropic_8k*10 之类配置描述）。
- MS MARCO 等开放域检索语料，用作模拟器检索增强的数据来源。





## RAG Search







## RAG Long-text and Memory 

**HippoRAG: Neurobiologically Inspired Long-Term Memory for Large Language Models** \
*Bernal Jiménez Gutiérrez, Yiheng Shu, Yu Gu, Michihiro Yasunaga, Yu Su* \
arXiv - May 2024 [[paper](https://arxiv.org/abs/2405.14831)] [[GitHub](https://github.com/OSU-NLP-Group/HippoRAG)]

**核心想法**
提出 HippoRAG，从海马体启发设计 LLM 的长期记忆机制，将信息编码到持久记忆库中，并通过生物启发的记忆检索策略支持长上下文、多轮交互中的信息重用，提高长文本任务的性能和记忆可靠性。​

**关键方法**

- 构建长期记忆模块，将对话或文档转为向量并存储在可扩展的记忆库中，采用神经生物启发的压缩与索引方式提升检索效率与信号保真。
- 在推理时使用专门的记忆检索策略选择历史关键信息注入 LLM，上下文窗口外的信息通过记忆系统回读，而非依赖原生上下文长度。

**主要数据集**
论文在多种长文本 QA 与多轮对话设置上评估（具体基准在正文表格中，如长文档 QA、对话记忆任务等），展示在长期依赖场景中的优势。





**Understanding Retrieval Augmentation for Long-Form Question Answering** \
*Hung-Ting Chen, Fangyuan Xu, Shane A. Arora, Eunsol Choi* \
arXiv - Oct 2023 [[Paper](https://arxiv.org/abs/2310.12150)]

**核心想法**
系统分析检索增强在长篇问答中的作用，研究不同检索策略、上下文配置对模型答案质量与证据利用方式的影响，为长文 RAG 中如何设计检索组件提供经验结论。​

**关键方法**

- 构建“ideal retrieval”等设置，分析在检索完美或受限情况下，生成模型如何使用上下文，量化检索质量与生成性能关系。
- 在多个长篇知识密集任务上对比不同检索数量、截断方式和段落选择策略，揭示信息冗余、噪声和覆盖率对回答质量的影响。

**主要数据集**

- 4 个长篇知识密集生成任务 / 数据集（如长篇 QA、传记或维基长文问答等，具体名称列在论文实验部分）。





**CAG【缓存即知识】**

> **缓存即知识**：像一位把常用参考书提前复印并装订成册的老教授，不再临时翻图书馆，而是直接把整本“知识”塞进 32K/64K 长上下文 LLM 的 KV-Cache。遇到提问，模型无需检索，直接从缓存里秒读原文，既省延迟又避幻觉，把 RAG 的“查-拼-答”三步曲简化成一步“读-答”。
>

* 时间：2025.02.23
* 论文：[Don't Do RAG: When Cache-Augmented Generation is All You Need for Knowledge Tasks](https://arxiv.org/abs/2412.15605)

Cache-Augmented Generation（CAG）把知识库一次性离线编码进长上下文 LLM 的 KV-Cache，推理阶段零检索、零延迟，直接读缓存输出答案。在 SQuAD 与 HotPotQA 多档规模实验中，CAG 的 BERTScore 均优于或持平传统 BM25/Dense RAG，且消除检索开销。





**AlayaDB【长程记忆库】**

> **长程记忆库**：像一座为超大模型量身定制的“智能图书馆”，把原本散落在 GPU 显存里的 KV-Cache 和检索索引统一搬进高性能分块存储层，实现“冷热分层、按需取阅”。在实验验证的 192 K token 级别上下文下，系统仍能在毫秒级定位并加载关键片段，既显著节省显存又保持与全注意力相当的精度，让长文本推理像翻书一样流畅。

* 时间：2025.04.14
* 论文：[AlayaDB: The Data Foundation for Efficient and Effective Long-context LLM Inference](https://arxiv.org/abs/2504.10326)

AlayaDB 提出面向长上下文 LLM 的统一数据底座：通过 KV-Cache 分块、向量索引与专用缓冲管理协同，将冷 KV 数据下沉至 CPU/SSD，热数据常驻 GPU；支持动态稀疏注意力查询（DIPR）、窗口缓存与上下文复用。实验在 ∞-Bench 43K–192K token 任务上，与全注意力相比 TTFT 最高提速 42×。AlayaDB 能够在保证服务等级目标（SLO）的同时，实现长上下文 LLM 推理的低资源消耗与高生成质量。





**RetrievalAttention【闪念检索】**

> **闪念检索**：像一位过目不忘的速记高手，面对超长文本时不再逐字细读，而是先凭“关键词闪念”瞬间锁定 KV 向量，再精读对应片段即可作答。通过把 KV-Cache 切成向量索引并做近似检索，它让长上下文 LLM 的推理既省显存又提速，实现毫秒级“大海捞针”。
>

* 时间：2024.12.31
* 论文：[RetrievalAttention: Accelerating Long-Context LLM Inference via Vector Retrieval](https://arxiv.org/abs/2409.10516)

RetrievalAttention将长序列的 KV-Cache 压缩成可检索的向量仓库，推理时先用轻量级检索器挑出 Top-k 关键 token 的 KV，再让注意力只在这 k 个 token 上计算，复杂度从 O(n²) 降到 O(k·n)。实验显示在 128k 上下文任务上，解码延迟比 Flat/IVF 分别再降 4.9× 与 1.98×，显存占用仅为全 KV 的 1–3%，精度与全注意力持平。





**Rethinking Memory in AI【记忆建筑师】**

> **记忆建筑师**：像一位系统化的档案管理员，把 AI 的“记忆”拆成“表示-操作-主题”三层架构，再给出六大原子操作的形式化公式，最后把记忆能力映射到四大研究主题，为下一代智能体提供可扩展、可解释、可演化的长期记忆蓝图。
>

* 时间：2025.05.27
* 论文：[Rethinking Memory in AI: Taxonomy, Operations, Topics, and Future Directions](https://arxiv.org/abs/2505.00675)

论文把 AI 记忆重新梳理为“表示-操作-主题”三层架构：先区分参数记忆与情境记忆，再定义 Consolidation 等六大原子操作，随后将能力映射到长期记忆、长上下文、参数修改、多源融合四大主题，并盘点 50+ 数据集、90+ 方法与 Replika 到 Mem0 的完整工具链，最终指出统一评估、KV效率、跨模态冲突与终身学习仍是未来突破口。





**RetroInfer【时光压缩器】**

> **时光压缩器**：像一位把长篇连续剧剪成高能集锦的导演，先离线把整段 KV 历史切成“关键帧”存入向量仓库，推理时只回放与问题最相关的几幕，既保留剧情精髓，又把显存和算力开销压到最低，让超长上下文模型也能“秒回”任意时刻的细节。
>

* 时间：2025.06.30
* 论文：[RetroInfer: A Vector-Storage Approach for Scalable Long-Context LLM Inference](https://arxiv.org/abs/2505.02922)

RetroInfer 将 KV-Cache 视为向量仓库，提出 Wave Index（注意力感知向量索引）与 Wave Buffer（异构内存调度器）。推理时先用 Wave Index 的三区近似（稳态区 + 检索区 + 估计区）挑出最关键 token，再让注意力仅在 1.8% KV 上精确计算，其余用质心估计，复杂度从 O(n²) 降到 O(k·n)。实验显示在长上下文任务上，解码吞吐比全注意力提升 4.5×，比现有稀疏注意力再快 10.5×，显存占用<5%，精度与全注意力持平。





**MemoRAG【过目不忘】**

> **过目不忘**：它不只是按需查找资料，而是已经把整个知识库都深入理解并记在心里。当你问问题时，它能快速从这个"超级大脑"中调取相关记忆，给出既准确又富有见地的答案，就像一个博学多识的专家。
>

* 时间：2024.09.01
* 项目：[https://github.com/qhjqhj00/MemoRAG](https://github.com/qhjqhj00/MemoRAG)
* 参考：[https://mp.weixin.qq.com/s/88FTElcYf5PIgHN0J8R7DA](https://mp.weixin.qq.com/s/88FTElcYf5PIgHN0J8R7DA)

MemoRAG是一个创新的检索增强生成（RAG）框架，构建在一个高效的超长记忆模型之上。与主要处理具有明确信息需求查询的标准RAG不同，MemoRAG利用其记忆模型实现对整个数据库的全局理解。通过从记忆中召回特定于查询的线索，MemoRAG增强了证据检索，从而产生更准确且具有丰富上下文的响应生成。





## RAG Evaluation

**ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems** \
*Jon Saad-Falcon, Omar Khattab, Christopher Potts, Matei Zaharia* \
arXiv - Nov 2023. [[Paper](https://arxiv.org/abs/2311.09476)] [[Github](https://github.com/stanford-futuredata/ares)]

提出 ARES，一个自动化 RAG 评测框架，通过自生成训练数据微调小模型评审器，对 RAG 的“上下文相关性、答案忠实度、答案相关性”三个维度打分，只需少量人工标注就能在多任务上稳定逼近人工评估效果。

**关键方法**

- 通过合成 query–passage–answer 数据训练轻量 LM judge，对检索到的上下文是否相关、答案是否忠实于上下文、以及与问题的相关性进行分类或评分。
- 使用 Prediction-Powered Inference (PPI) 将少量人工标注与自动评估结合，给出有统计保证的性能估计。
- 将 RAG 系统拆解为组件（检索、重排序、生成）分别评估，支持跨任务、跨领域泛化的评审器。

**主要数据集**

- 8 个知识密集任务，来自 KILT、SuperGLUE 和 AIS 等基准，作为被评估的 RAG 任务集合。
- 合成数据集由框架自动生成，用于训练评审模型本身。





**RAGChecker【质检员】**

> **质检员**：不只简单地判断答案对错，而是会深入检查整个回答过程中的每个环节，从资料查找到最终答案生成，就像一个严格的考官，既给出详细的评分报告，还会指出具体哪里需要改进。
>

* 时间：2024.08.15
* 论文：[RAGChecker: A Fine-grained Framework for Diagnosing Retrieval-Augmented Generation](https://arxiv.org/abs/2408.08067)
* 项目：[https://github.com/amazon-science/RAGChecker](https://github.com/amazon-science/RAGChecker)
* 参考：[https://mp.weixin.qq.com/s/x4o7BinnwvTsOa2_hegcrQ](https://mp.weixin.qq.com/s/x4o7BinnwvTsOa2_hegcrQ)

RAGChecker 的诊断工具为 RAG 系统提供细粒度、全面、可靠的诊断报告，并为进一步提升性能，提供可操作的方向。它不仅能评估系统的整体表现，还能深入分析检索和生成两大核心模块的性能。





**RAGViz【透视眼】**

> **透视眼**：让RAG系统变透明，看得见模型在读哪句话，像医生看X光片一样，哪里不对一目了然。
>

* 时间：2024.11.04
* 论文：[RAGViz: Diagnose and Visualize Retrieval-Augmented Generation](https://arxiv.org/abs/2411.01751)
* 项目：[https://github.com/cxcscmu/RAGViz](https://github.com/cxcscmu/RAGViz)
* 参考：[https://mp.weixin.qq.com/s/ZXvAWDhqKRPq1u9NTfYFnQ](https://mp.weixin.qq.com/s/ZXvAWDhqKRPq1u9NTfYFnQ)

RAGViz提供了对检索文档和模型注意力的可视化，帮助用户理解生成的标记与检索文档之间的交互，可用于诊断和可视化RAG系统。





**AstuteRAG【明智判官】**

> **明智判官**：对外部信息保持警惕，不轻信检索结果，善用自身积累的知识，甄别信息真伪，像资深法官一样，权衡多方证据定论。
>

* 时间：2024.10.09
* 论文：[Astute RAG: Overcoming Imperfect Retrieval Augmentation and Knowledge Conflicts for Large Language Models](https://arxiv.org/abs/2410.07176)
* 参考：[https://mp.weixin.qq.com/s/Y8ozl3eH1osJTNOSuu4v1w](https://mp.weixin.qq.com/s/Y8ozl3eH1osJTNOSuu4v1w)

通过适应性地从LLMs内部知识中提取信息，结合外部检索结果，并根据信息的可靠性来最终确定答案，从而提高系统的鲁棒性和可信度。







## RAG Optimization

**Learning to Filter Context for Retrieval-Augmented Generation** \
*Zhiruo Wang, Jun Araki, Zhengbao Jiang, Md Rizwan Parvez, Graham Neubig* \
arxiv- Nov 2023 [[Paper](https://arxiv.org/abs/2311.08377)][[Github](https://github.com/zorazrw/filco)] 

**核心想法（摘要浓缩）**
针对检索结果包含大量无关或有害上下文的问题，提出 FILCO，通过词面与信息论方法识别“有用上下文”，训练上下文过滤模型，在推理时只向生成模型提供经过筛选的段落，从而在多个知识密集任务上改善结果并减轻幻觉。​

**关键方法**

- 使用字符串包含、词面匹配和基于 mutual information 的指标估计段落对目标输出的“增益”，构建银标签，训练专门的 context-filter 模型 M_ctx。
- 推理时由 M_ctx 从检索到的段落中选择子集再交给生成模型 M_gen，即 M_gen(o | M_ctx(q, P) ⊕ q)。
- 在 FLAN-T5 和 LLaMA2 上实验证明，该筛选策略在抽取式 QA、多跳 / 长文 QA、事实验证和对话生成等任务上均优于直接用全部检索文本或其它启发式方法。

**主要数据集**

- 6 个 Wikipedia 支持的知识密集任务数据集，涵盖：
  - Extractive QA：如 Natural Questions (NQ)、TriviaQA 等。
  - Complex / long-form QA：如 HotpotQA、WikiHop 或类似多跳任务。
  - Fact verification 与对话生成任务，对应公开的维基证据数据集和知识对话语料。





**Large Language Models Can Be Easily Distracted by Irrelevant Context** \
*Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed Chi, Nathanael Schärli, Denny Zhou* \
ICML 2023 - Jan 2023 [[Paper](https://arxiv.org/abs/2302.00093)][[Github](https://github.com/google-research-datasets/GSM-IC)] 

**核心想法（摘要浓缩）**
系统地展示：给 LLM 输入中添加与任务无关或对立的上下文，会显著扰乱模型推理，尤其在数学和逻辑任务上；说明现有模型在分辨“相关 vs. 无关”信息方面存在明显缺陷。​

**关键方法**

- 设计 controlled settings，在原始题目前后加入不同类型的干扰上下文（无关解释、错误提示等），测量模型准确率下降。
- 分析不同模型规模与指令调优方法在“抗干扰性”上的差异，体现单纯增大规模并不能完全解决问题。
- 提供开源的 GSM-IC 数据集，把 GSM8K 等数学题系统性添加干扰上下文，用于后续研究模型鲁棒性。

**主要数据集**

- 基于 GSM8K 构造的 GSM-IC（GSM with Irrelevant Context）数学推理数据集。
- 其它逻辑 /推理任务数据，用于测试在不同任务类别中的干扰敏感度（详见原文实验节）。





**Evidentiality-guided Generation for Knowledge-Intensive NLP Tasks** \
*Akari Asai, Matt Gardner, Hannaneh Hajishirzi* \
NAACL 2022 - Dec 2021 [[Paper](https://arxiv.org/abs/2112.08688)][[Github](https://github.com/akariasai/evidentiality_qa)] 

**核心想法（摘要浓缩）**
针对 RAG 模型可能从“无证据但带有虚假线索”的段落学习到错误模式的问题，提出“evidentiality-guided generation”：联合训练生成任务与 passage evidentiality 预测任务，并用任务无关的银标签挖掘方法标注证据性，从而在多个知识密集任务上提升性能并减少记忆化与幻觉。​

**关键方法**

- 多任务学习框架：主任务生成答案，辅任务预测每个检索段落是否真正提供支持证据（evidentiality），联合训练生成器。
- 提出 task-agnostic 的“银 evidentiality 标注”方法，对缺少人工证据标注的语料自动构造高质量标签。
- 对比直接用全部检索段落训练的生成器，证明 evidentiality signal 有助于模型学会关注真正的证据而非 spurious cues。

**主要数据集**
五个数据集，涵盖三类任务：​

- 开放域 QA：Natural Questions Open、TriviaQA。
- 事实验证：FEVER 等标准基准。
- 知识增强对话：如 MultiDoc2Dial 等 knowledge-grounded dialogue 数据集。





**When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories** \
*Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, Hannaneh Hajishirzi* \
ACL 2023 - Dec 2022 [[Paper](https://arxiv.org/abs/2212.10511)][[Github](https://github.com/alextmallen/adaptive-retrieval)] 

**核心想法（摘要浓缩）**
构建 PopQA 数据集、大规模探测 10 个模型和 4 种检索增强方法，分析 LMs 在“记忆事实知识”上的强弱。结果显示：模型对冷门实体记忆很差；单纯 scale up 对长尾知识帮助有限，而检索增强在长尾上显著提升；同时非参数记忆在热门实体上可能误导模型。基于此提出按需检索的简单策略以提升性能并降低推理成本。​

**关键方法**

- 构造 PopQA：14k 开放域问答，按实体流行度分桶，用于系统性分析“热门 vs 长尾”知识记忆情况。
- 比较不同大小模型与 4 种 RAG 方法在 PopQA 上的表现，量化规模、检索与知识流行度的交互效应。
- 设计一个条件检索策略：只有在需要时才调用非参数记忆，兼顾性能与推理成本，并减少在热门实体上被检索误导的概率。

**主要数据集**

- PopQA：新构建的 14k 开放域 QA 数据集，主体由维基实体问题组成，并标注实体流行度。
- 其它标准 QA 基准（如 NQ、TriviaQA 等）用于对比或辅助分析。





**Multi-Meta-RAG【元筛选器】**

> **元筛选器**：像个经验丰富的资料管理员，通过多重筛选机制，从海量信息中精准定位最相关的内容。它不只看表面，还会深入分析文档的"身份标签"（元数据），确保找到的每份资料都真正对题。
>

* 时间：2024.06.19
* 论文：[Multi-Meta-RAG: Improving RAG for Multi-Hop Queries using Database Filtering with LLM-Extracted Metadata](https://arxiv.org/abs/2406.13213)
* 项目：[https://github.com/mxpoliakov/multi-meta-rag](https://github.com/mxpoliakov/multi-meta-rag)
* 参考：[https://mp.weixin.qq.com/s/Jf3qdFR-o_A4FXwmOOZ3pg](https://mp.weixin.qq.com/s/Jf3qdFR-o_A4FXwmOOZ3pg)

Multi-Meta-RAG使用数据库过滤和LLM提取的元数据来改进RAG从各种来源中选择与问题相关的相关文档。





**RankRAG【全能选手】**

> **全能选手**：通过一点特训就能当好"评委"和"选手"双重角色。像个天赋异禀的运动员，只需要少量指导就能在多个项目上超越专业选手，还能把看家本领都融会贯通。
>

* 时间：2024.07.02
* 论文：[RankRAG: Unifying Context Ranking with Retrieval-Augmented Generation in LLMs](https://arxiv.org/abs/2407.02485)
* 参考：[https://mp.weixin.qq.com/s/BZDXCTKSKLOwDv1j8_75_Q](https://mp.weixin.qq.com/s/BZDXCTKSKLOwDv1j8_75_Q)

RankRAG的通过指令微调单一的LLM，使其同时具备上下文排名和答案生成的双重功能。通过在训练数据中加入少量排序数据，经过指令微调的大语言模型效果出奇地好，甚至超过了现有的专家排序模型，包括在大量排序数据上专门微调的相同大语言模型。这种设计不仅简化了传统RAG系统中多模型的复杂性，还通过共享模型参数增强了上下文的相关性判断和信息的利用效率。





**SFR-RAG【精简检索】**

> **精简检索**：像个精练的参考顾问，体积虽小但功能精准，既能理解需求又懂得寻求外部帮助，保证回答既准确又高效。

* 时间：2024.09.16
* 论文：[SFR-RAG: Towards Contextually Faithful LLMs](https://arxiv.org/abs/2409.09916)
* 参考：[https://mp.weixin.qq.com/s/rArOICbHpkmFPR5UoBIi5A](https://mp.weixin.qq.com/s/rArOICbHpkmFPR5UoBIi5A)

SFR-RAG是一个经过指令微调的小型语言模型，重点是基于上下文的生成和最小化幻觉。通过专注于在保持高性能的同时减少参数数量，SFR-RAG模型包含函数调用功能，使其能够与外部工具动态交互以检索高质量的上下文信息。





## RAG Application

**Deficiency of Large Language Models in Finance: An Empirical Examination of Hallucination** \
*Haoqiang Kang, Xiao-Yang Liu* \
arXiv - Nov 2023 [[Paper](https://arxiv.org/abs/2311.15548)] 

**核心想法（摘要浓缩）**
系统评估主流 LLM 在金融领域的幻觉问题，展示在金融问答与分析任务中，闭源与开源模型都存在严重事实错误与不可靠推理，并探讨在该高度敏感领域部署 LLM 的风险与改进方向（包括使用检索增强等）。​

**关键方法**

- 设计一套金融任务（如公司财务问答、法规解释、市场事件分析等），对多种 LLM 的回答进行事实性与一致性评估。
- 分析不同提示、工具接入（如检索）对减轻 hallucination 的作用，揭示仅依赖参数记忆在金融场景不可行。

**主要数据集**
基于真实金融文件与公开市场信息构造的问答或分析数据集（具体由作者自建，包含年报、法规等材料）。





**Clinfo.ai: An Open-Source Retrieval-Augmented Large Language Model System for Answering Medical Questions using Scientific Literature** \
*Alejandro Lozano, Scott L Fleming, Chia-Chun Chiang, Nigam Shah* \
arXiv - Oct 2023. [[Paper](https://arxiv.org/abs/2310.16146v1)] 

**核心想法（摘要浓缩）**
提出 Clinfo.ai，一套开源 Web 系统，用 RAG 流水线回答临床问题，动态检索医学文献，并给出可溯源的摘要。论文同时提出基于系统综述构建的新评测任务 PubMedRS-200，并报告 Clinfo.ai 与其它 OpenQA 系统在该基准上的表现。​

**关键方法**

- 将 LLM 与向量数据库、PubMed 检索 API 和计算工具集成，形成 end-to-end 医学问答与摘要工作流。
- 明确评测任务：给定临床问题，从文献中检索并综合回答，强调 factuality 和 evidence linkage。
- 以 Clinfo.ai 与其他公开 OpenQA 系统在相同管线下对比，展示 RAG 在医学场景中的优势与局限。

**主要数据集**

- PubMedRS-200：从系统综述构造的 200 个医疗问题–答案–参考文献三元组，用于评估医学 RAG 系统。
- 在线 PubMed 文献库，用作系统检索的知识源。





**PEARL: Personalizing Large Language Model Writing Assistants with Generation-Calibrated Retrievers** \
*Sheshera Mysore, Zhuoran Lu, Mengting Wan, Longqi Yang, Steve Menezes, Tina Baghaee, Emmanuel Barajas Gonzalez, Jennifer Neville, Tara Safavi* \
arXiv - Nov 2023. [[Paper](https://arxiv.org/abs/2311.09180)] 

**核心想法（摘要浓缩）**
PEARL 聚焦“个性化写作助手”，提出 generation-calibrated retriever：使检索模块与生成器协同，以检索到更符合用户风格与需求的文档 / 片段，从而提高个性化写作质量。​

**关键方法**

- 通过用户历史写作与偏好构建个性化检索空间，联合优化检索器和生成器，使检索结果既相关又可被生成模块有效利用。
- 设计多种个性化写作任务（如邮件、社媒文案等），比较通用 RAG 与 PEARL 检索的差异。

**主要数据集**
多源用户写作语料（公司内部或众包收集，包含邮件、评论等），构建成个性化写作任务数据集，用于训练和评估个性化 RAG 助手。





**GraphRAG-Local-UI【改装师】**

> **改装师**：把跑车改装成适合本地道路的实用车，加装了友好的仪表盘，让人人都能轻松驾驶。
>

* 时间：2024.07.14
* 项目：[https://github.com/severian42/GraphRAG-Local-UI](https://github.com/severian42/GraphRAG-Local-UI)
* 参考：[https://mp.weixin.qq.com/s/DLvF7YpU3IfWvnu9ZyiBIA](https://mp.weixin.qq.com/s/DLvF7YpU3IfWvnu9ZyiBIA)

GraphRAG-Local-UI是基于Microsoft的GraphRAG的本地模型适配版本，具有丰富的交互式用户界面生态系统。





**Nano-GraphRAG【轻装上阵】**

> **轻装上阵**：像个轻装上阵的运动员，把繁复的装备都简化了，但保留了核心能力。
>

* 时间：2024.07.25
* 项目：[https://github.com/gusye1234/nano-graphrag](https://github.com/gusye1234/nano-graphrag)
* 参考：[https://mp.weixin.qq.com/s/pnyhz0jA4jgLndMUM9IU1g](https://mp.weixin.qq.com/s/pnyhz0jA4jgLndMUM9IU1g)

Nano-GraphRAG是一个更小、更快、更简洁的 GraphRAG，同时保留了核心功能。





**RAGFlow-GraphRAG【导航员】**

> **导航员**：在问答的迷宫里开辟捷径，先画张地图把知识点都标好，重复的路标合并掉，还特地给地图瘦身，让问路的人不会绕远路。
>

* 时间：2024.08.02
* 项目：[https://github.com/infiniflow/ragflow](https://github.com/infiniflow/ragflow)
* 参考：[https://mp.weixin.qq.com/s/c5-0dCWI0bIa2zHagM1w0w](https://mp.weixin.qq.com/s/c5-0dCWI0bIa2zHagM1w0w)

RAGFlow借鉴了GraphRAG的实现，在文档预处理阶段，引入知识图谱构建作为可选项，服务于QFS问答场景，并引入了实体去重、Token优化等改进。





**RAGViz【透视眼】**

> **透视眼**：让RAG系统变透明，看得见模型在读哪句话，像医生看X光片一样，哪里不对一目了然。
>

* 时间：2024.11.04
* 论文：[RAGViz: Diagnose and Visualize Retrieval-Augmented Generation](https://arxiv.org/abs/2411.01751)
* 项目：[https://github.com/cxcscmu/RAGViz](https://github.com/cxcscmu/RAGViz)
* 参考：[https://mp.weixin.qq.com/s/ZXvAWDhqKRPq1u9NTfYFnQ](https://mp.weixin.qq.com/s/ZXvAWDhqKRPq1u9NTfYFnQ)

RAGViz提供了对检索文档和模型注意力的可视化，帮助用户理解生成的标记与检索文档之间的交互，可用于诊断和可视化RAG系统。





**RAGLAB【竞技场】**

> **竞技场**：让各种算法可以在相同的规则下进行公平竞争和比较，就像科学实验室里的标准化测试流程，确保每个新方法都能得到客观透明的评估。
>

* 时间：2024.08.21
* 论文：[RAGLAB: A Modular and Research-Oriented Unified Framework for Retrieval-Augmented ](https://arxiv.org/abs/2408.11381)Generation
* 项目：[https://github.com/fate-ubw/RAGLab](https://github.com/fate-ubw/RAGLab)
* 参考：[https://mp.weixin.qq.com/s/WSk0zdWZRXMVvm4-_HiFRw](https://mp.weixin.qq.com/s/WSk0zdWZRXMVvm4-_HiFRw)

新型RAG算法之间越来越缺乏全面和公平的比较，开源工具的高级抽象导致缺乏透明度，并限制了开发新算法和评估指标的能力。RAGLAB是一个模块化、研究导向的开源库，重现6种算法并构建全面研究生态。借助RAGLAB，我们在10个基准上公平对比6种算法，助力研究人员高效评估和创新算法。





**Kotaemon【乐高】**

> **乐高**：一套现成的问答积木套装，既能直接拿来用，又能自由拆装改造。用户要用就用，开发要改就改，随心所欲不失章法。
>

* 时间：2024.05.15
* 项目：[https://github.com/Cinnamon/kotaemon](https://github.com/Cinnamon/kotaemon)
* 参考：[https://mp.weixin.qq.com/s/SzoE2Hb82a6yUU7EcfF5Hg](https://mp.weixin.qq.com/s/SzoE2Hb82a6yUU7EcfF5Hg)

一个开源的干净且可定制的RAG UI，用于构建和定制自己的文档问答系统。既考虑了最终用户的需求，也考虑了开发者的需求。





**FlashRAG【百宝箱】**

> **百宝箱**：把各路RAG神器打包成一个工具包，让研究者像挑选积木一样，随心所欲地搭建自己的检索模型。
>

* 时间：2024.05.22
* 论文：[FlashRAG: A Modular Toolkit for Efficient Retrieval-Augmented Generation Research](https://arxiv.org/abs/2405.13576)
* 项目：[https://github.com/RUC-NLPIR/FlashRAG](https://github.com/RUC-NLPIR/FlashRAG)
* 参考：[https://mp.weixin.qq.com/s/vvOdcARaU1LD6KgcdShhoA](https://mp.weixin.qq.com/s/vvOdcARaU1LD6KgcdShhoA)

FlashRAG是一个高效且模块化的开源工具包，旨在帮助研究人员在统一框架内重现现有的RAG方法并开发他们自己的RAG算法。我们的工具包实现了12种先进的RAG方法，并收集和整理了32个基准数据集。





**Medical-Graph-RAG【数字医生】**

> **数字医生**：像个经验丰富的医学顾问，用图谱把复杂的医疗知识整理得清清楚楚，诊断建议不是凭空想象，而是有理有据，让医生和患者都能看明白每个诊断背后的依据。
>

* 时间：2024.08.08
* 论文：[Medical Graph RAG: Towards Safe Medical Large Language Model via Graph Retrieval-Augmented Generation](https://arxiv.org/abs/2408.04187)
* 项目：[https://github.com/SuperMedIntel/Medical-Graph-RAG](https://github.com/SuperMedIntel/Medical-Graph-RAG)
* 参考：[https://mp.weixin.qq.com/s/5mX-hCyFdve98H01x153Eg](https://mp.weixin.qq.com/s/5mX-hCyFdve98H01x153Eg)

MedGraphRAG 是一个框架，旨在解决在医学中应用 LLM 的挑战。它使用基于图谱的方法来提高诊断准确性、透明度并集成到临床工作流程中。该系统通过生成由可靠来源支持的响应来提高诊断准确性，解决了在大量医疗数据中维护上下文的困难。





**LA-RAG【方言通】**

> **方言通**：像个精通各地方言的语言专家，通过细致的语音分析和上下文理解，不仅能准确识别标准普通话，还能听懂带有地方特色的口音，让AI与不同地区的人都能无障碍交流。
>

* 时间：2024.09.13
* 论文：[LA-RAG:Enhancing LLM-based ASR Accuracy with Retrieval-Augmented Generation](https://arxiv.org/abs/2409.08597)
* 参考：[https://mp.weixin.qq.com/s/yrmtBqP4bmQ2wYZM7F24Yg](https://mp.weixin.qq.com/s/yrmtBqP4bmQ2wYZM7F24Yg)

LA-RAG，是一种基于LLM的ASR的新型检索增强生成（RAG）范例。LA-RAG 利用细粒度标记级语音数据存储和语音到语音检索机制，通过 LLM 上下文学习 (ICL) 功能提高 ASR 准确性。  





**TableRAG【Excel专家】**

> **Excel专家**：不只简单地查看表格数据，而是懂得从表头和单元格两个维度去理解和检索数据，就像熟练使用数据透视表一样，能快速定位和提取所需的关键信息。
>

* 时间：2024.10.07
* 论文：[TableRAG: Million-Token Table Understanding with Language Models](https://arxiv.org/abs/2410.04739)
* 参考：[https://mp.weixin.qq.com/s/n0iu6qOufc1izlzuRjQO6g](https://mp.weixin.qq.com/s/n0iu6qOufc1izlzuRjQO6g)

TableRAG专为表格理解设计了检索增强生成框架，通过查询扩展结合Schema和单元格检索，能够在提供信息给语言模型之前精准定位关键数据，从而实现更高效的数据编码和精确检索，大幅缩短提示长度并减少信息丢失。





**HtmlRAG【排版师】**

> **排版师**：把知识不是当作流水账来记，而是像排版杂志一样，该加粗的加粗，该标红的标红。就像一个挑剔的美编，觉得光有内容不够，还得讲究排版，这样重点才能一目了然。
>

* 时间：2024.11.05
* 论文：[HtmlRAG: HTML is Better Than Plain Text for Modeling Retrieved Knowledge in RAG Systems](https://arxiv.org/abs/2411.02959)
* 项目：[https://github.com/plageon/HtmlRAG](https://github.com/plageon/HtmlRAG)
* 参考：[https://mp.weixin.qq.com/s/1X6k9QI71BIyQ4IELQxOlA](https://mp.weixin.qq.com/s/1X6k9QI71BIyQ4IELQxOlA)

HtmlRAG在RAG中使用HTML而不是纯文本作为检索知识的格式，在对外部文档中的知识进行建模时，HTML比纯文本更好，并且大多数LLM具有强大的理解HTML的能力。HtmlRAG提出了HTML清理、压缩和修剪策略，以缩短HTML同时最小化信息损失。





**Class-RAG【法官】**

> **法官**：不是靠死板的条文判案，而是通过不断扩充的判例库来研判。像个经验老到的法官，手握活页法典，随时翻阅最新案例，让判决既有温度又有尺度。
>

* 时间：2024.10.18
* 论文：[Class-RAG: Content Moderation with Retrieval Augmented Generation](https://arxiv.org/abs/2410.14881)
* 参考：[https://mp.weixin.qq.com/s/4AfZodMGJ5JQ2NUCFt3eqQ](https://mp.weixin.qq.com/s/4AfZodMGJ5JQ2NUCFt3eqQ)

内容审核分类器对生成式 AI 的安全性至关重要。然而，安全与不安全内容间的细微差别常令人难以区分。随着技术广泛应用，持续微调模型以应对风险变得愈发困难且昂贵。为此，我们提出 Class-RAG 方法，通过动态更新检索库，实现即时风险缓解。与传统微调模型相比，Class-RAG 更具灵活性与透明度，且在分类与抗攻击方面表现更佳。研究还表明，扩大检索库能有效提升审核性能，成本低廉。





## Graph Based RAG

**GraphRAG【社区摘要】**

> **社区摘要**：先把小区居民的关系网理清楚，再给每个邻里圈做个简介。有人问路时，各个邻里圈提供线索，最后整合成最完整的答案。
>

* 时间：2024.04.24
* 论文：[From Local to Global: A Graph RAG Approach to Query-Focused Summarization](https://arxiv.org/abs/2404.16130)
* 项目：[https://github.com/microsoft/graphrag](https://github.com/microsoft/graphrag)
* 参考：[https://mp.weixin.qq.com/s/I_-rpMNVoQz-KvUlgQH-2w](https://mp.weixin.qq.com/s/I_-rpMNVoQz-KvUlgQH-2w)

GraphRAG分两个阶段构建基于图的文本索引：首先从源文档中推导出实体知识图，然后为所有紧密相关实体的组预生成社区摘要。给定一个问题，每个社区摘要用于生成部分响应，然后在向用户的最终响应中再次总结所有部分响应。





**AntGroup-GraphRAG【百家之长】**

> **百家之长**：汇集行业百家之长，擅用多种方式快速定位信息，既能提供精准检索，又能理解自然语言查询，让复杂的知识检索变得既经济又高效。
>

* 时间：2024.05.16
* 项目：[https://github.com/eosphoros-ai/DB-GPT](https://github.com/eosphoros-ai/DB-GPT)
* 参考：[https://mp.weixin.qq.com/s/LfhAY91JejRm_A6sY6akNA](https://mp.weixin.qq.com/s/LfhAY91JejRm_A6sY6akNA)

蚂蚁TuGraph团队基于DB-GPT构建的开源GraphRAG框架，兼容了向量、图谱、全文等多种知识库索引底座，支持低成本的知识抽取、文档结构图谱、图社区摘要与混合检索以解决QFS问答问题。另外也提供了关键词、向量和自然语言等多样化的检索能力支持。





**GRAG【侦探】**

> **侦探**：不满足于表面线索，深入挖掘文本之间的关联网络，像破案一样追踪每条信息背后的真相，让答案更准确。
>

* 时间：2024.05.26
* 论文：[GRAG: Graph Retrieval-Augmented Generation](https://arxiv.org/abs/2405.16506)
* 项目：[https://github.com/HuieL/GRAG](https://github.com/HuieL/GRAG)
* 参考：[https://mp.weixin.qq.com/s/xLVaFVr7rnYJq0WZLsFVMw](https://mp.weixin.qq.com/s/xLVaFVr7rnYJq0WZLsFVMw)

传统RAG模型在处理复杂的图结构数据时忽视了文本之间的联系和数据库的拓扑信息，从而导致了性能瓶颈。GRAG通过强调子图结构的重要性，显著提升了检索和生成过程的性能并降低幻觉。





**Camel-GraphRAG【左右开弓】**

> **左右开弓**：一只眼睛用Mistral扫描文本提取情报，另只眼睛用Neo4j编织关系网。查找时左右眼配合，既能找相似的，又能顺着线索图追踪，让搜索更全面精准。
>

* 时间：2024.05.27
* 项目：[https://github.com/camel-ai/camel](https://github.com/camel-ai/camel)
* 参考：[https://mp.weixin.qq.com/s/DhnAd-k-CtdGFVrwGat90w](https://mp.weixin.qq.com/s/DhnAd-k-CtdGFVrwGat90w)

Camel-GraphRAG依托Mistral模型提供支持，从给定的内容中提取知识并构建知识结构，然后将这些信息存储在 Neo4j图数据库中。随后采用一种混合方法，将向量检索与知识图谱检索相结合，来查询和探索所存储的知识。





**G-RAG【串门神器】**

> **串门神器**：不再是单打独斗地查资料，而是给每个知识点都建立人际关系网。像个社交达人，不仅知道每个朋友的特长，还清楚谁和谁是酒肉朋友，找答案时直接顺藤摸瓜。

* 时间：2024.05.28
* 论文：[Don't Forget to Connect! Improving RAG with Graph-based Reranking](https://arxiv.org/abs/2405.18414)
* 参考：[https://mp.weixin.qq.com/s/e6sRpYFDTQ7w7ituIjyovQ](https://mp.weixin.qq.com/s/e6sRpYFDTQ7w7ituIjyovQ)

RAG 在处理文档与问题上下文的关系时仍存在挑战，当文档与问题的关联性不明显或仅包含部分信息时，模型可能无法有效利用这些文档。此外，如何合理推断文档之间的关联也是一个重要问题。 G-RAG实现了RAG检索器和阅读器之间基于图神经网络（GNN）的重排器。该方法结合了文档之间的连接信息和语义信息（通过抽象语义表示图），为 RAG 提供了基于上下文的排序器。





**LLM-Graph-Builder【搬运工】**

> **搬运工**：给混乱的文字安个明白的家。不是简单地搬运，而是像个强迫症患者，把每个知识点都贴上标签，画上关系线，最后在Neo4j的数据库里盖起一座井井有序的知识大厦。
>

* 时间：2024.05.29
* 项目：[https://github.com/neo4j-labs/llm-graph-builder](https://github.com/neo4j-labs/llm-graph-builder)
* 参考：[https://mp.weixin.qq.com/s/9Jy11WH7UgrW37281XopiA](https://mp.weixin.qq.com/s/9Jy11WH7UgrW37281XopiA)

Neo4j开源的基于LLM提取知识图谱的生成器，可以把非结构化数据转换成Neo4j中的知识图谱。利用大模型从非结构化数据中提取节点、关系及其属性。





**Meta-Knowledge-RAG【学者】**

> **学者**：像个学术界的资深研究员，不仅收集资料，还会主动思考问题，为每份文档做批注和总结，甚至预先设想可能的问题。它会把相关的知识点串联起来，形成知识网络，让查询变得更有深度和广度，就像有一个学者在帮你做研究综述。
>

* 时间：2024.08.16
* 论文：[Meta Knowledge for Retrieval Augmented Large Language Models](https://arxiv.org/abs/2408.09017)
* 参考：[https://mp.weixin.qq.com/s/twFVKQDTRZTGvDeYA8-c0A](https://mp.weixin.qq.com/s/twFVKQDTRZTGvDeYA8-c0A)

Meta-Knowledge-RAG（MK Summary）引入了一种新颖的以数据为中心的 RAG 工作流程，将传统的 “检索-读取” 系统转变为更先进的 “准备-重写-检索-读取” 框架，以实现对知识库的更高领域专家级理解。我们的方法依赖于为每个文档生成元数据和合成的问题与答案以及为基于元数据的文档集群引入元知识摘要的新概念。所提出的创新实现了个性化的用户查询增强和跨知识库的深度信息检索。





**CommunityKG-RAG【社群探索】**

> **社群探索**：像个熟悉社区关系网络的向导，善于利用知识间的关联和群组特征，在不需要特别学习的情况下，就能准确地找到相关信息，并验证其可靠性。
>

* 时间：2024.08.16
* 论文：[CommunityKG-RAG: Leveraging Community Structures in Knowledge Graphs for Advanced Retrieval-Augmented Generation in Fact-Checking](https://arxiv.org/abs/2408.08535)
* 参考：[https://mp.weixin.qq.com/s/ixKV-PKf8ohqZDCTN9jLZQ](https://mp.weixin.qq.com/s/ixKV-PKf8ohqZDCTN9jLZQ)

CommunityKG-RAG是一种新颖的零样本框架，它将知识图谱中的社区结构与RAG系统相结合，以增强事实核查过程。CommunityKG-RAG能够在无需额外训练的情况下适应新的领域和查询，它利用知识图谱中社区结构的多跳性质，显著提高信息检索的准确性和相关性。





**SubgraphRAG【定位仪】**

> **定位仪**：不是漫无目的地大海捞针，而是精准绘制一张小型知识地图，让 AI 能快速找到答案。
>

* 时间：2024.10.28
* 论文：[Simple is Effective: The Roles of Graphs and Large Language Models in Knowledge-Graph-Based Retrieval-Augmented Generation](https://arxiv.org/abs/2410.20724)
* 项目：[https://github.com/Graph-COM/SubgraphRAG](https://github.com/Graph-COM/SubgraphRAG)
* 参考：[https://mp.weixin.qq.com/s/ns22XLKsABly7RjpSjQ_Fw](https://mp.weixin.qq.com/s/ns22XLKsABly7RjpSjQ_Fw)

SubgraphRAG扩展了基于KG的RAG框架，通过检索子图并利用LLM进行推理和答案预测。将轻量级多层感知器与并行三元组评分机制相结合，以实现高效灵活的子图检索，同时编码有向结构距离以提高检索有效性。检索到的子图大小可以灵活调整，以匹配查询需求和下游LLM的能力。这种设计在模型复杂性和推理能力之间取得了平衡，实现了可扩展且通用的检索过程。





**AGENTiGraph【知识管家】**

> **知识管家**：像个善于对话的图书管理员，通过日常交流帮你整理和展示知识，带着一队助手随时准备解答问题、更新资料，让知识管理变得简单自然。
>

* 时间：2024.10.15
* 论文：[AGENTiGraph: An Interactive Knowledge Graph Platform for LLM-based Chatbots Utilizing Private Data](https://arxiv.org/abs/2410.11531)
* 参考：[https://mp.weixin.qq.com/s/iAlcxjXHlz7xfwVd4lpQ-g](https://mp.weixin.qq.com/s/iAlcxjXHlz7xfwVd4lpQ-g)

AGENTiGraph通过自然语言交互进行知识管理的平台。它集成了知识提取、集成和实时可视化。AGENTiGraph 采用多智能体架构来动态解释用户意图、管理任务和集成新知识，确保能够适应不断变化的用户需求和数据上下文。





**FastGraphRAG【雷达】**

> **雷达**：像谷歌网页排名一样，给知识点也排出个热度榜。就好比社交网络中的意见领袖，越多人关注就越容易被看见。它不是漫无目的地搜索，而是像个带着雷达的侦察兵，哪里的信号强就往哪里看。
>

* 时间：2024.10.23
* 项目：[https://github.com/circlemind-ai/fast-graphrag](https://github.com/circlemind-ai/fast-graphrag)
* 参考：[https://mp.weixin.qq.com/s/uBcYaO5drTUabcCXh3bzjA](https://mp.weixin.qq.com/s/uBcYaO5drTUabcCXh3bzjA)

FastGraphRAG提供了一个高效、可解释且精度高的快速图检索增强生成（FastGraphRAG）框架。它将PageRank算法应用于知识图谱的遍历过程，快速定位最相关的知识节点。通过计算节点的重要性得分，PageRank使GraphRAG能够更智能地筛选和排序知识图谱中的信息。这就像是为GraphRAG装上了一个"重要性雷达"，能够在浩如烟海的数据中快速定位关键信息。





**AGENTiGraph【知识管家】**

> **知识管家**：像个善于对话的图书管理员，通过日常交流帮你整理和展示知识，带着一队助手随时准备解答问题、更新资料，让知识管理变得简单自然。
>

* 时间：2024.10.15
* 论文：[AGENTiGraph: An Interactive Knowledge Graph Platform for LLM-based Chatbots Utilizing Private Data](https://arxiv.org/abs/2410.11531)
* 参考：[https://mp.weixin.qq.com/s/iAlcxjXHlz7xfwVd4lpQ-g](https://mp.weixin.qq.com/s/iAlcxjXHlz7xfwVd4lpQ-g)

AGENTiGraph通过自然语言交互进行知识管理的平台。它集成了知识提取、集成和实时可视化。AGENTiGraph 采用多智能体架构来动态解释用户意图、管理任务和集成新知识，确保能够适应不断变化的用户需求和数据上下文。
